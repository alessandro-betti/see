\input colordvi
\magnification=\magstep1
\baselineskip14pt
\tracingpages=1 % show paging decisions, to help with figure placement
\baselineskip=12pt minus .1pt % allow tiny bit of flexibility

\def\today{\ifcase\month\or
  January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or December\fi
  \space\number\day, \number\year}

\newcount\twodigits
\def\hours{\twodigits=\time \divide\twodigits by 60 \printtwodigits
  \multiply\twodigits by-60 \advance\twodigits by\time
  :\printtwodigits}

\def\gobbleone1{}
\def\printtwodigits{\advance\twodigits100
  \expandafter\gobbleone\number\twodigits
  \advance\twodigits-100 }
  
\def\frac#1#2{{#1\over #2}}
\def\feat{C\mskip -10mu\lower-2pt\hbox{\fiverm 1}\,}
\def\dts{\mathinner{\ldotp\ldotp}}
\def\ast{\mathop{\hbox{\lower 1.5pt\hbox{$\buildrel x\over *$}}}}
\def\Ast#1{\mathop{\hbox{\lower 1.5pt\hbox{$\buildrel #1\over *$}}}}

\font\ninerm=cmr9
\font\eightrm=cmr8

%star heading
\font\manfnt=manfnt % special symbols from the TeX project
\font\titlefont=cmssbx10 scaled\magstep2
\def\volheadline#1{\line{\cleaders\hbox{\raise3pt\hbox{\manfnt\char'36}}\hfill
       \titlefont\ #1\ \cleaders\hbox{\raise3pt\hbox{\manfnt\char'36}}\hfill}}
%end

% macros for verbatim scanning
% the max number of \tt chars/line is 66 (10pt), 73 (9pt), 81 (8pt), 93 (7pt)
% minus 4 or 5 characters for indentation at the left
\chardef\other=12
\def\ttverbatim{\catcode`\\=\other
  \catcode`\{=\other
  \catcode`\}=\other
  \catcode`\$=\other
  \catcode`\&=\other
  \catcode`\#=\other
  \catcode`\%=\other
  \catcode`\~=\other
  \catcode`\_=\other
  \catcode`\^=\other
  \obeyspaces \obeylines \tt}
\def\begintt{$$\ttverbatim \catcode`\|=0 \ttfinish}
{\catcode`\|=0 \catcode`|\=\other % | is temporary escape character
  |obeylines   % end of line is active
  |gdef|intt#1^^M{|noalign{#1}}%
  |gdef|ttfinish#1^^M#2\endtt{#1|let^^M=|cr %
    |halign{|hskip|parindent##|hfil|cr#2}$$}}

\line{{\bf Built-in probabilistic normalization}\hfil{\eightrm
rough notes, \today\/ @ \hours}}
\bigskip
\bigskip\begingroup\narrower\narrower
\noindent We reformulate our theory (arXiv:1801.07110)
with a built-in probabilistic normalization.
We therefore eliminate the two terms
that enforces normalization in the Lagrangian at the cost of
a strong non-linearity in the definition of the features in terms of the
filters.
\par\endgroup

\bigskip
\bigskip\noindent
Let $\sigma\colon {\bf R}^n\to {\bf R}^n$ a normalizing function; for the
moment we will assume that:
\medskip
\item{1. } $\sigma\in C^s({\bf R}^n)$, where $s$ is some appropriate degree
of smoothness ($s\ge1$);
\item{2. } the function $\sigma$ has the probabilistic normalization
property: Given any
$x\in {\bf R}^n$ we have that $\sigma_i(x)\ge0$ for all $i=1,\dots, n$ and
$\sum_{i=1}^n\sigma_i(x)=1$.
\item{3. } $\sigma_i(0)=1/n$
\medskip

So let us replace the definition of the features proposed in (arXiv:1801.07110)
with the following:
$$\feat_{ix}^\sharp(t):=\sigma_i\big(q\ast\Gamma(t)\big),
\qquad \bigl(q\ast\Gamma(t)\bigr)_i:=\sum_{j\in J_i}q_j\Gamma_{T_x(j)}.
\eqno(1)$$
Notice that when $\Gamma\equiv 0$ then $\feat_{ix}^\sharp(t)=\sigma_i(0)=1/n$
(by property 3 above). Let us also define the {\it activation function\/}
$a^x\colon {\bf R}\to {\bf R}^n$ on the point $x$ by
$a^x_i(t):=(q\ast\Gamma(t))_i$. With this notation Eq.~(1) becomes
$\feat_{ix}^\sharp(t)=\sigma_i(a^x(t))$.

The functional we are now interested in is
$${\cal A}(q)={\cal A}_0(q)+{\cal H}(q)+{\cal V}_m(q),\eqno(2)$$
where ${\cal A}_0$ is the regularization term defined in Section~5 and 
${\cal V}_m(q)$ is the invariance-under-motion term that is identical to the
one described in the technical report. Clearly here we are making the strong
assumption that we impose the invariance on the activation rather than
on the features; invariance of the activation implies
invariance of the features the converse, however, it is not true.

The remaining term $\cal H$ in Eq.~(2) is {\it minus\/} the mutual
information. This is the only term that is rewritten with the built-in
normalization:
$${\cal H}(q)={1\over2}
\sum_{i=0}^{n-1}\biggl(\int_0^T dt\, h(t)\sum_{x\in X^\sharp}
g_x \sigma_i (a_x(t))\biggr)^2-{\lambda_C\over2}
\sum_{i=0}^{n-1}\int_0^T dt\, h(t)\sum_{x\in X^\sharp}
g_x \bigl(\sigma_i (a_x(t)\bigr)^2.\eqno(3)$$
As usual we approximate the entropy to make it local in time, then
Eq.~(3) becomes 
$${\cal H}(q)=
\sum_{i=0}^{n-1}\int_0^T dt\, h(t)\biggl[{1\over2}\biggl(\sum_{x\in X^\sharp}
g_x \sigma_i(q\ast\Gamma(t))\biggr)^2-{\lambda_C\over2}
\sum_{x\in X^\sharp} g_x \Bigl(\sigma_i\bigl(q\ast\Gamma(t)\bigr)
\Bigr)^2\biggr].\eqno(4)$$
Now we need to compute the variation of this new term; notice however that
this expression does not depend on the derivatives of $q$, therefore is a
potential-like term.

Define
$$\Phi(t,q):=h(t)\sum_{i=0}^{n-1}\biggl[{1\over2}\biggl(\sum_{x\in X^\sharp}
g_x \sigma_i(q\ast\Gamma(t))\biggr)^2-{\lambda_C\over2}
\sum_{x\in X^\sharp} g_x \Bigl(\sigma_i\bigl(q\ast\Gamma(t)\bigr)
\Bigr)^2\biggr],\eqno(5)$$
so that ${\cal H}(q)=\int dt\, \Phi(t,q(t))$. Then the variation of this term
gives contributes in the ELE with the term $\nabla_q\Phi(t,q)$. This term is
highly non-linear in $q$.

Euler-Lagrange equation reduces to 
$$\eqalign{ \hat \alpha
q^{(4)}+2\dot{\hat\alpha}q^{(3)}+(\ddot{\hat\alpha}+\dot{\hat\gamma}-\hat
\beta-\lambda_M\hat M^\natural ) \ddot q &-\left(\dot{\hat
\beta}-\ddot{\hat\gamma}+\lambda_M\bigl(\dot{\hat M^\natural}-\hat
N^\natural+(\hat N^\natural)'\bigr)\right)\dot q\cr &+\left(\hat k
+\lambda_M\bigl(\hat O^\natural-(\dot{\hat N^\natural})'\bigr)
\right)q +\nabla_q \Phi(t,q)=0.}\eqno(6)$$
This is only apparently a linear differential equation because of the term
$\nabla_q\Phi$.

More explicitly we have that
$${\partial \Phi\over\partial q_k}=\sum_{i,\ell=0}^{n-1}\sum_{j\in J_\ell}
\sum_{x\in X^\sharp}g_x\left(\biggl(\sum_{y\in X^\sharp}
g_y \sigma_i(q\Ast y\Gamma(t))\biggr)-\lambda_C\sigma_i(q\ast \Gamma(t))
\right){\partial \sigma_i\over\partial a_\ell^x}\delta_{jk}\Gamma_{T_x(j)}.
\eqno(7)$$
Is this form useful at all?

\medskip
\noindent
{\bf Reduction to gradient.\enspace} Consider Eq.~(6) and consider
the following choice of parameters:
$$\lambda_M=\alpha=\beta=0, \quad \gamma=\theta^{-2}\quad {\rm and}\quad
\theta\gg 1;$$
with this choice this equation becomes
$$\dot q=-{1\over h}\nabla_q \Phi(t,q),$$
which is clearly a gradient descent method.

\bigskip\bigskip
\volheadline{NEW NOTATION}
\bigskip
\noindent
We know from our technical report that the activations are evaluated as
$$a^x_i=\sum_{k=0}^{m-1}\sum_{-f\le\xi_1,\xi_2\le f}
\varphi_{ik(x_1-\xi_1)(x_2-\xi_2)}C_{k\xi_1\xi_2}=\sum_{k=0}^{m-1}
\sum_{-f\le\xi_1,\xi_2\le f}\varphi_{ik\xi_1\xi_2}C_{k(x_1-\xi_1)(x_2-\xi_2)}.
$$
The last equality is true since the convolution is invertible when the
filters are defined on $[-2L\dts 2L]$ where $L$ is the size of the frame.
Here we use the convention that whenever the last two indexes of
$C$ are outside $[0\dts L]\times[0\dts L]$, $C$ is set by definition to $0$
($0$-padding).
Define the following $m f^2\times n$ matrix:
$$Q:=\pmatrix{\varphi_{0000}&\varphi_{1000}&\dots&\varphi_{(n-1)000}\cr
\varphi_{0001}&\varphi_{1001}&\dots&\varphi_{100(n-1)}\cr
\vdots&\vdots&&\vdots\cr
\varphi_{0(m-1)ff}&\varphi_{1(m-1)ff}&\dots&\varphi_{(n-1)(m-1)ff}\cr}$$
Then define $\Gamma$ as the matrix that has a line for each value of
$(x_1,x_2)$ then it will have $\ell^2$ lines and on the columns the values of
$C_{k(x_1-\xi_1)(x_2-\xi_2)}$, so it will have $mf^2$ columns. The order of
the lines and columns is chosen in such a way that $a_i^x=(\Gamma Q)_{xi}$.
With this notation we have that the features can be expressed as
$$C^\sharp_{ix}(t)=\sigma_i(a_0^x,a_2^x,\dots, a_{n-1}^x),\qquad
x=0,\dots, \ell^2-1$$
What we want to find then are the elements of the matrix  $Q$.

With this notation we can rewrite Eq.~(4) as
$${\cal H}(Q)=
\sum_{i=0}^{n-1}\int_0^T dt\, h(t)\biggl[{1\over2}
\biggl(\,\sum_{x=0}^{\ell^2-1}
g_x \sigma_i(a^x)\biggr)^2-{\lambda_C\over2}
\sum_{x=0}^{\ell^2-1} g_x \Bigl(\sigma_i(a^x)
\Bigr)^2\biggr],$$
where we have defined $a^x=(a_0^x,a_2^x,\dots, a_{n-1}^x)$, and then
$$\Phi(t,Q):=h(t)\sum_{i=0}^{n-1}\biggl[{1\over2}
\biggl(\,\sum_{x=0}^{\ell^2-1}
g_x \sigma_i(a^x)\biggr)^2-{\lambda_C\over2}
\sum_{x=0}^{\ell^2-1} g_x \Bigl(\sigma_i(a^x)
\Bigr)^2\biggr]=h(t)(\Phi_1-\lambda_C\Phi_2).$$
The derivative with respect to the generic element $Q_{km}$ yields:
$$
{\partial\Phi(t,Q)\over\partial Q_{km}}=
h(t)\sum_{i=0}^{n-1}\sum_{x=0}^{\ell^2-1} g_x\biggl(
\sum_{y=0}^{\ell^2-1}g_y\sigma_i(a^y)-\lambda_C\sigma_i(a^x)\biggr)
\bigl(\sigma_i(a^x)\delta_{im}-\sigma_i(a^x)\sigma_m(a^x)\bigr)\Gamma_{xk}.
$$
Let us also define $S_{xi}=\sigma_i(a^x)$ 
$$\eqalign{
{\partial\Phi_1(t,Q)\over\partial Q_{km}}&=
\sum_{i=0}^{n-1}\sum_{x=0}^{\ell^2-1} g_x(S'g)_i
\biggl(S_{xi}\delta_{im}
-S_{xi}S_{xm}\biggr)
\Gamma_{xk}\cr
&=((g^\leftrightarrow\odot\Gamma)'(S\odot (S'g)^{\updownarrow}))_{km}
-\sum_{i=0}^{n-1}\sum_{x=0}^{\ell^2-1}g_x(S'g)_iS_{xi}S_{xm}\Gamma_{xk}\cr
&=((g^\leftrightarrow\odot\Gamma)'(S\odot (S'g)^{\updownarrow}))_{km}-
(\Gamma'[(g\odot(SS'g))^\leftrightarrow\odot S])_{km},\cr}$$
where $\odot$ is the Hadamard product between vectors or matrices
and given a vector $v$ we indicate with
$v^\leftrightarrow$ ($v^\updownarrow$)
the matrix which has on the columns (rows) the vector $v$ (and has
consistent dimensions).
$$\eqalign{
{\partial\Phi_2(t,Q)\over\partial Q_{km}}&=
\sum_{i=0}^{n-1}\sum_{x=0}^{\ell^2-1} g_x S_{xi}
\biggl(S_{xi}\delta_{im}
-S_{xi}S_{xm}\biggr)\Gamma_{xk}\cr
&=(\Gamma'(g^\leftrightarrow\odot (S\odot S))_{km}
-\sum_{i=0}^{n-1}\sum_{x=0}^{\ell^2-1}g_x(S\odot S)_{xi}S_{xm}\Gamma_{xk}\cr
&=(\Gamma'(g^\leftrightarrow\odot (S\odot S))_{km}-
(\Gamma'[(g\odot(S\odot S)^-)^\leftrightarrow\odot S])_{km},\cr}$$
where given a matrix $A$ we denote with $A^-$ the column vector obtained from
$A$ by adding the columns together: $(A^-)_i=\sum_j A_{ij}$.
Then putting everything together we eventually end up with:
$$\eqalign{
{1\over h}{\partial\Phi\over\partial Q}=&
(g^\leftrightarrow\odot\Gamma)'(S\odot (S'g)^{\updownarrow})-
\Gamma'[(g\odot(SS'g))^\leftrightarrow\odot S]\cr
&-\lambda_C\big(\Gamma'(g^\leftrightarrow\odot (S\odot S))-
\Gamma'[(g\odot(S\odot S)^-)^\leftrightarrow\odot S]
\big).\cr}$$
This form has been checked with the following  Matlab script; we highlight
in red the changes.
\begintt
| clear all;

| % parameters
| wh = 10;
| vol = 9;
| m = 3;
| lambdaC = 1.0;
 
| % main matrices and vectors
| C = randn(wh,vol);
| Q = randn(vol,m);
| A = C*Q;
| S = exp(A) ./ (sum(exp(A),2));
| g = repmat(1/wh,wh,1);
 
| % custom matrices and vectors
| eA = exp(A);
 
| % case a |textRed
| a1 = (repmat(g,1,vol).*C)'*(S.*repmat((S'*g)',wh,1))...
|       -C'*(repmat(g.*(S*(S'*g)),1,m).*S);
| a2 = C'*(repmat(g,1,m).*(S.*S))-C'*(repmat(g.*sum(S.*S,2),1,m).*S);
| a = (a1 - lambdaC*a2);
|textBlack 
| % case b
| b = zeros(vol,m);|textRed
| gs=zeros(m,1);
| for i=1:m
| for y = 1:wh
|     gs(i) = gs(i)+g(y)*S(y,i);
| end
| end|textBlack
 
| for k = 1:vol
|     for z = 1:m
|         for i = 1:m
|             for x = 1:wh
|                 b(k,z) = b(k,z) + g(x) ...
|                     * (gs(i)-lambdaC*S(x,i)) ...|textRed
|                     * (double(i==z)*S(x,i)-S(x,i)*S(x,z)) ...|textBlack
|                     * C(x,k);
|             end
|         end
|     end
| end
 
| % compare cases
| mc = max(max(abs(a-b)));
| sc = (sum(sum((a > 0) .* (b > 0))) + sum(sum((a < 0) .* (b < 0))))/(vol*m);
| fprintf("Max diff = %f, Coherent sign = %f percent\n", mc, sc*100);
\endtt

We can now continue and show how all the other terms can be
rewritten with the same notation.

\bye