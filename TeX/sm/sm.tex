\magnification=\magstep1
\baselineskip14pt
\tracingpages=1 % show paging decisions, to help with figure placement
\baselineskip=12pt minus .1pt % allow tiny bit of flexibility

\def\today{\ifcase\month\or
  January\or February\or March\or April\or May\or June\or
  July\or August\or September\or October\or November\or December\fi
  \space\number\day, \number\year}

\newcount\twodigits
\def\hours{\twodigits=\time \divide\twodigits by 60 \printtwodigits
  \multiply\twodigits by-60 \advance\twodigits by\time
  :\printtwodigits}

\def\gobbleone1{}
\def\printtwodigits{\advance\twodigits100
  \expandafter\gobbleone\number\twodigits
  \advance\twodigits-100 }
  
\def\frac#1#2{{#1\over #2}}
\def\feat{C\mskip -10mu\lower-2pt\hbox{\fiverm 1}\,}
\def\dts{\mathinner{\ldotp\ldotp}}
\def\ast{\mathop{\hbox{\lower 1.5pt\hbox{$\buildrel x\over *$}}}}
\def\Ast#1{\mathop{\hbox{\lower 1.5pt\hbox{$\buildrel #1\over *$}}}}

\font\ninerm=cmr9
\font\eightrm=cmr8

%star heading
\font\manfnt=manfnt % special symbols from the TeX project
\font\titlefont=cmssbx10 scaled\magstep2
\def\volheadline#1{\line{\cleaders\hbox{\raise3pt\hbox{\manfnt\char'36}}\hfill
       \titlefont\ #1\ \cleaders\hbox{\raise3pt\hbox{\manfnt\char'36}}\hfill}}
%end

\line{{\bf Built-in probabilistic normalization}\hfil{\eightrm
rough notes, \today\/ @ \hours}}
\bigskip
\bigskip\begingroup\narrower\narrower
\noindent We reformulate our theory (arXiv:1801.07110)
with a built-in probabilistic normalization.
We therefore eliminate the two terms
that enforces normalization in the Lagrangian at the cost of
a strong non-linearity in the definition of the features in terms of the
filters.
\par\endgroup

\bigskip
\bigskip\noindent
Let $\sigma\colon {\bf R}^n\to {\bf R}^n$ a normalizing function; for the
moment we will assume that:
\medskip
\item{1. } $\sigma\in C^s({\bf R}^n)$, where $s$ is some appropriate degree
of smoothness ($s\ge1$);
\item{2. } the function $\sigma$ has the probabilistic normalization
property: Given any
$x\in {\bf R}^n$ we have that $\sigma_i(x)\ge0$ for all $i=1,\dots, n$ and
$\sum_{i=1}^n\sigma_i(x)=1$.
\item{3. } $\sigma_i(0)=1/n$
\medskip

So let us replace the definition of the features proposed in (arXiv:1801.07110)
with the following:
$$\feat_{ix}^\sharp(t):=\sigma_i\big(q\ast\Gamma(t)\big),
\qquad \bigl(q\ast\Gamma(t)\bigr)_i:=\sum_{j\in J_i}q_j\Gamma_{T_x(j)}.
\eqno(1)$$
Notice that when $\Gamma\equiv 0$ then $\feat_{ix}^\sharp(t)=\sigma_i(0)=1/n$
(by property 3 above). Let us also define the {\it activation function\/}
$a^x\colon {\bf R}\to {\bf R}^n$ on the point $x$ by
$a^x_i(t):=(q\ast\Gamma(t))_i$. With this notation Eq.~(1) becomes
$\feat_{ix}^\sharp(t)=\sigma_i(a^x(t))$.

The functional we are now interested in is
$${\cal A}(q)={\cal A}_0(q)+{\cal H}(q)+{\cal V}_m(q),\eqno(2)$$
where ${\cal A}_0$ is the regularization term defined in Section~5 and 
${\cal V}_m(q)$ is the invariance-under-motion term that is identical to the
one described in the technical report. Clearly here we are making the strong
assumption that we impose the invariance on the activation rather than
on the features; invariance of the activation implies
invariance of the features the converse, however, it is not true.

The remaining term $\cal H$ in Eq.~(2) is {\it minus\/} the mutual
information. This is the only term that is rewritten with the built-in
normalization:
$${\cal H}(q)={1\over2}
\sum_{i=0}^{n-1}\biggl(\int_0^T dt\, h(t)\sum_{x\in X^\sharp}
g_x \sigma_i (a_x(t))\biggr)^2-{\lambda_C\over2}
\sum_{i=0}^{n-1}\int_0^T dt\, h(t)\sum_{x\in X^\sharp}
g_x \bigl(\sigma_i (a_x(t)\bigr)^2.\eqno(3)$$
As usual we approximate the entropy to make it local in time, then
Eq.~(3) becomes 
$${\cal H}(q)=
\sum_{i=0}^{n-1}\int_0^T dt\, h(t)\biggl[{1\over2}\biggl(\sum_{x\in X^\sharp}
g_x \sigma_i(q\ast\Gamma(t))\biggr)^2-{\lambda_C\over2}
\sum_{x\in X^\sharp} g_x \Bigl(\sigma_i\bigl(q\ast\Gamma(t)\bigr)
\Bigr)^2\biggr].\eqno(4)$$
Now we need to compute the variation of this new term; notice however that
this expression does not depend on the derivatives of $q$, therefore is a
potential-like term.

Define
$$\Phi(t,q):=h(t)\sum_{i=0}^{n-1}\biggl[{1\over2}\biggl(\sum_{x\in X^\sharp}
g_x \sigma_i(q\ast\Gamma(t))\biggr)^2-{\lambda_C\over2}
\sum_{x\in X^\sharp} g_x \Bigl(\sigma_i\bigl(q\ast\Gamma(t)\bigr)
\Bigr)^2\biggr],\eqno(5)$$
so that ${\cal H}(q)=\int dt\, \Phi(t,q(t))$. Then the variation of this term
gives contributes in the ELE with the term $\nabla_q\Phi(t,q)$. This term is
highly non-linear in $q$.

Euler-Lagrange equation reduces to 
$$\eqalign{ \hat \alpha
q^{(4)}+2\dot{\hat\alpha}q^{(3)}+(\ddot{\hat\alpha}+\dot{\hat\gamma}-\hat
\beta-\lambda_M\hat M^\natural ) \ddot q &-\left(\dot{\hat
\beta}-\ddot{\hat\gamma}+\lambda_M\bigl(\dot{\hat M^\natural}-\hat
N^\natural+(\hat N^\natural)'\bigr)\right)\dot q\cr &+\left(\hat k
+\lambda_M\bigl(\hat O^\natural-(\dot{\hat N^\natural})'\bigr)
\right)q +\nabla_q \Phi(t,q)=0.}\eqno(6)$$
This is only apparently a linear differential equation because of the term
$\nabla_q\Phi$.

More explicitly we have that
$${\partial \Phi\over\partial q_k}=\sum_{i,\ell=0}^{n-1}\sum_{j\in J_\ell}
\sum_{x\in X^\sharp}g_x\left(\biggl(\sum_{y\in X^\sharp}
g_y \sigma_i(q\Ast y\Gamma(t))\biggr)-\lambda_C\sigma_i(q\ast \Gamma(t))
\right){\partial \sigma_i\over\partial a_\ell^x}\delta_{jk}\Gamma_{T_x(j)}.
\eqno(7)$$
Is this form useful at all?

\bigskip\bigskip
\volheadline{NEW NOTATION}
\bigskip
\noindent
We know from our technical report that the activations are evaluated as
$$a^x_i=\sum_{k=0}^{m-1}\sum_{-f\le\xi_1,\xi_2\le f}
\varphi_{ik(x_1-\xi_1)(x_2-\xi_2)}C_{k\xi_1\xi_2}=\sum_{k=0}^{m-1}
\sum_{-f\le\xi_1,\xi_2\le f}\varphi_{ik\xi_1\xi_2}C_{k(x_1-\xi_1)(x_2-\xi_2)}.
$$
The last equality is true since the convolution is invertible when the
filters are defined on $[-2L\dts 2L]$ where $L$ is the size of the frame.
Here we use the convention that whenever the last two indexes of
$C$ are outside $[0\dts L]\times[0\dts L]$, $C$ is set by definition to $0$
($0$-padding).
Define the following $m f^2\times n$ matrix:
$$Q:=\pmatrix{\varphi_{0000}&\varphi_{1000}&\dots&\varphi_{(n-1)000}\cr
\varphi_{0001}&\varphi_{1001}&\dots&\varphi_{100(n-1)}\cr
\vdots&\vdots&&\vdots\cr
\varphi_{0(m-1)ff}&\varphi_{1(m-1)ff}&\dots&\varphi_{(n-1)(m-1)ff}\cr}$$
Then define $\Gamma$ as the matrix that has a line for each value of
$(x_1,x_2)$ then it will have $\ell^2$ lines and on the columns the values of
$C_{k(x_1-\xi_1)(x_2-\xi_2)}$, so it will have $mf^2$ columns. The order of
the lines and columns is chosen in such a way that $a_i^x=(\Gamma Q)_{xi}$.
With this notation we have that the features can be expressed as
$$C^\sharp_{ix}(t)=\sigma_i(a_0^x,a_2^x,\dots, a_{n-1}^x),\qquad
x=0,\dots, \ell^2-1$$
What we want to find then are the elements of the matrix  $Q$.

With this notation we can rewrite Eq.~(4) as
$${\cal H}(Q)=
\sum_{i=0}^{n-1}\int_0^T dt\, h(t)\biggl[{1\over2}
\biggl(\,\sum_{x=0}^{\ell^2-1}
g_x \sigma_i(a^x)\biggr)^2-{\lambda_C\over2}
\sum_{x=0}^{\ell^2-1} g_x \Bigl(\sigma_i(a^x)
\Bigr)^2\biggr],$$
where we have defined $a^x=(a_0^x,a_2^x,\dots, a_{n-1}^x)$, and then
$$\Phi(t,Q):=h(t)\sum_{i=0}^{n-1}\biggl[{1\over2}
\biggl(\,\sum_{x=0}^{\ell^2-1}
g_x \sigma_i(a^x)\biggr)^2-{\lambda_C\over2}
\sum_{x=0}^{\ell^2-1} g_x \Bigl(\sigma_i(a^x)
\Bigr)^2\biggr].$$
The derivative with respect to the generic element $Q_{km}$ yields:
$$
{\partial\Phi(t,Q)\over\partial Q_{km}}=
h(t)\sum_{i=0}^{n-1}\sum_{x=0}^{\ell^2-1} g_x\biggl(
\sum_{y=0}^{\ell^2-1}g_y\sigma_i(a^y)-\lambda_C\sigma_i(a^x)\biggr)
\biggl({\delta_{im}\over \sum_{\alpha=0}^{n-1}e^{a^x_\alpha}}
-{e^{a_i^x+a_m^x}\over \big(\sum_{\alpha=0}^{n-1}e^{a^x_\alpha}\big)^2 }\biggr)
\Gamma_{xk}.
$$
\bye
